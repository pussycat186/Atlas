name: ATLAS Prism Production Gates

on:
  workflow_dispatch:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: "20"
  PNPM_VERSION: "9"

jobs:
  # Read LIVE_URLS and set environment variables
  setup-urls:
    runs-on: ubuntu-latest
    outputs:
      proof-url: ${{ steps.read-urls.outputs.proof-url }}
      admin-url: ${{ steps.read-urls.outputs.admin-url }}
      dev-url: ${{ steps.read-urls.outputs.dev-url }}
      gateway-url: ${{ steps.read-urls.outputs.gateway-url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Read LIVE_URLS
        id: read-urls
        run: |
          if [[ -f LIVE_URLS.json ]]; then
            PROOF_URL=$(jq -r '.frontends.proof_messenger // "https://atlas-proof-messenger.vercel.app"' LIVE_URLS.json)
            ADMIN_URL=$(jq -r '.frontends.admin_insights // "https://atlas-admin-insights.vercel.app"' LIVE_URLS.json)
            DEV_URL=$(jq -r '.frontends.dev_portal // "https://atlas-dev-portal.vercel.app"' LIVE_URLS.json)
            GATEWAY_URL=$(jq -r '.backends.gateway // "https://atlas-gateway.sonthenguyen186.workers.dev"' LIVE_URLS.json)
          else
            PROOF_URL="https://atlas-proof-messenger.vercel.app"
            ADMIN_URL="https://atlas-admin-insights.vercel.app"
            DEV_URL="https://atlas-dev-portal.vercel.app"
            GATEWAY_URL="https://atlas-gateway.sonthenguyen186.workers.dev"
          fi
          
          echo "proof-url=$PROOF_URL" >> $GITHUB_OUTPUT
          echo "admin-url=$ADMIN_URL" >> $GITHUB_OUTPUT
          echo "dev-url=$DEV_URL" >> $GITHUB_OUTPUT
          echo "gateway-url=$GATEWAY_URL" >> $GITHUB_OUTPUT
          
          echo "Production URLs:"
          echo "  Proof Messenger: $PROOF_URL"
          echo "  Admin Insights: $ADMIN_URL"
          echo "  Dev Portal: $DEV_URL"
          echo "  Gateway: $GATEWAY_URL"

  # Basic SKU Tests
  test-basic:
    runs-on: ubuntu-latest
    needs: setup-urls
    env:
      PROOF_URL: ${{ needs.setup-urls.outputs.proof-url }}
      ADMIN_URL: ${{ needs.setup-urls.outputs.admin-url }}
      DEV_URL: ${{ needs.setup-urls.outputs.dev-url }}
      GATEWAY_URL: ${{ needs.setup-urls.outputs.gateway-url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Setup pnpm
        uses: pnpm/action-setup@v3
        with:
          version: ${{ env.PNPM_VERSION }}
          
      - name: Install dependencies
        run: pnpm install --no-frozen-lockfile
        
      - name: Install Playwright browsers
        run: npx playwright install --with-deps
        
      - name: Run Playwright Basic Tests
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          mkdir -p docs/evidence/$TIMESTAMP/playwright/basic
          
          # Set BASE_URL for Playwright
          export BASE_URL=$PROOF_URL
          
          npx playwright test tests/basic.spec.ts --config=playwright.config.ts --reporter=html
          cp -r playwright-report/* docs/evidence/$TIMESTAMP/playwright/basic/ || true
          
      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x
        
      - name: Run Lighthouse Basic Tests
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          mkdir -p docs/evidence/$TIMESTAMP/lighthouse/basic
          
          # Create LHCI config for Basic thresholds
          cat > .lighthouse/basic.lhci.json << EOF
          {
            "ci": {
              "collect": {
                "url": ["$PROOF_URL", "$ADMIN_URL", "$DEV_URL"],
                "numberOfRuns": 1,
                "settings": {
                  "preset": "desktop"
                }
              },
              "assert": {
                "assertions": {
                  "categories:performance": ["error", {"minScore": 0.90}],
                  "categories:accessibility": ["error", {"minScore": 0.95}],
                  "categories:best-practices": ["error", {"minScore": 0.95}],
                  "categories:seo": ["error", {"minScore": 0.95}]
                }
              },
              "upload": {
                "target": "filesystem",
                "outputDir": "docs/evidence/$TIMESTAMP/lighthouse/basic"
              }
            }
          }
          EOF
          
          lhci autorun --config=./.lighthouse/basic.lhci.json
          
      - name: Setup k6
        uses: grafana/setup-k6@v1
        
      - name: Run k6 Basic Tests
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          mkdir -p docs/evidence/$TIMESTAMP/k6/basic
          
          # Create k6 test script
          cat > tests/k6/basic.js << EOF
          import http from 'k6/http';
          import { check } from 'k6';
          import { Rate } from 'k6/metrics';

          const errorRate = new Rate('errors');

          export let options = {
            vus: 5,
            duration: '30s',
            thresholds: {
              'http_req_duration{class:implemented}': ['p(95)<200'],
              'http_req_failed{class:implemented}': ['rate<0.01'],
              'errors{class:implemented}': ['rate<0.01']
            }
          };

          export default function () {
            // Test implemented endpoints
            const healthResponse = http.get('$GATEWAY_URL/health', {
              tags: { class: 'implemented' }
            });
            check(healthResponse, {
              'health status is 200': (r) => r.status === 200,
              'health response time < 200ms': (r) => r.timings.duration < 200,
            }) || errorRate.add(1, { class: 'implemented' });

            // Test unimplemented endpoints (should not affect thresholds)
            const qtcaResponse = http.get('$GATEWAY_URL/qtca/tick', {
              tags: { class: 'unimplemented' }
            });
            check(qtcaResponse, {
              'qtca returns 404': (r) => r.status === 404,
            });
          }
          EOF
          
          k6 run tests/k6/basic.js --summary-export=docs/evidence/$TIMESTAMP/k6/basic/summary.json

  # Pro SKU Tests
  test-pro:
    runs-on: ubuntu-latest
    needs: setup-urls
    env:
      PROOF_URL: ${{ needs.setup-urls.outputs.proof-url }}
      ADMIN_URL: ${{ needs.setup-urls.outputs.admin-url }}
      DEV_URL: ${{ needs.setup-urls.outputs.dev-url }}
      GATEWAY_URL: ${{ needs.setup-urls.outputs.gateway-url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Setup pnpm
        uses: pnpm/action-setup@v3
        with:
          version: ${{ env.PNPM_VERSION }}
          
      - name: Install dependencies
        run: pnpm install --no-frozen-lockfile
        
      - name: Install Playwright browsers
        run: npx playwright install --with-deps
        
      - name: Run Playwright Pro Tests
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          mkdir -p docs/evidence/$TIMESTAMP/playwright/pro
          
          # Set BASE_URL for Playwright
          export BASE_URL=$PROOF_URL
          
          npx playwright test tests/pro.spec.ts --config=playwright.config.ts --reporter=html
          cp -r playwright-report/* docs/evidence/$TIMESTAMP/playwright/pro/ || true
          
      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x
        
      - name: Run Lighthouse Pro Tests
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          mkdir -p docs/evidence/$TIMESTAMP/lighthouse/pro
          
          # Create LHCI config for Pro thresholds
          cat > .lighthouse/pro.lhci.json << EOF
          {
            "ci": {
              "collect": {
                "url": ["$PROOF_URL", "$ADMIN_URL", "$DEV_URL"],
                "numberOfRuns": 1,
                "settings": {
                  "preset": "desktop"
                }
              },
              "assert": {
                "assertions": {
                  "categories:performance": ["error", {"minScore": 0.95}],
                  "categories:accessibility": ["error", {"minScore": 0.95}],
                  "categories:best-practices": ["error", {"minScore": 1.0}],
                  "categories:seo": ["error", {"minScore": 1.0}]
                }
              },
              "upload": {
                "target": "filesystem",
                "outputDir": "docs/evidence/$TIMESTAMP/lighthouse/pro"
              }
            }
          }
          EOF
          
          lhci autorun --config=./.lighthouse/pro.lhci.json
          
      - name: Setup k6
        uses: grafana/setup-k6@v1
        
      - name: Run k6 Pro Tests
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          mkdir -p docs/evidence/$TIMESTAMP/k6/pro
          
          # Create k6 test script
          cat > tests/k6/pro.js << EOF
          import http from 'k6/http';
          import { check } from 'k6';
          import { Rate } from 'k6/metrics';

          const errorRate = new Rate('errors');

          export let options = {
            vus: 10,
            duration: '60s',
            thresholds: {
              'http_req_duration{class:implemented}': ['p(95)<200'],
              'http_req_failed{class:implemented}': ['rate<0.01'],
              'errors{class:implemented}': ['rate<0.01']
            }
          };

          export default function () {
            // Test implemented endpoints
            const healthResponse = http.get('$GATEWAY_URL/health', {
              tags: { class: 'implemented' }
            });
            check(healthResponse, {
              'health status is 200': (r) => r.status === 200,
              'health response time < 200ms': (r) => r.timings.duration < 200,
            }) || errorRate.add(1, { class: 'implemented' });

            // Test metrics endpoint
            const metricsResponse = http.get('$GATEWAY_URL/metrics', {
              tags: { class: 'implemented' }
            });
            check(metricsResponse, {
              'metrics status is 200': (r) => r.status === 200,
            }) || errorRate.add(1, { class: 'implemented' });

            // Test unimplemented endpoints (should not affect thresholds)
            const qtcaResponse = http.get('$GATEWAY_URL/qtca/tick', {
              tags: { class: 'unimplemented' }
            });
            check(qtcaResponse, {
              'qtca returns 404': (r) => r.status === 404,
            });
          }
          EOF
          
          k6 run tests/k6/pro.js --summary-export=docs/evidence/$TIMESTAMP/k6/pro/summary.json

  # Deploy to Vercel (if enabled)
  deploy-frontends:
    runs-on: ubuntu-latest
    needs: [test-basic, test-pro]
    if: always() && needs.test-basic.result == 'success' && needs.test-pro.result == 'success'
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Setup pnpm
        uses: pnpm/action-setup@v3
        with:
          version: ${{ env.PNPM_VERSION }}
          
      - name: Install dependencies
        run: pnpm install --no-frozen-lockfile
        
      - name: Deploy Proof Messenger
        if: ${{ vars.VERCEL_ORG_ID != '' && secrets.VERCEL_TOKEN != '' }}
        working-directory: apps/proof-messenger
        run: |
          vercel pull --yes --environment=production --token ${{ secrets.VERCEL_TOKEN }}
          vercel build --token ${{ secrets.VERCEL_TOKEN }}
          vercel deploy --prebuilt --prod --token ${{ secrets.VERCEL_TOKEN }}
          
      - name: Deploy Admin Insights
        if: ${{ vars.VERCEL_ORG_ID != '' && secrets.VERCEL_TOKEN != '' }}
        working-directory: apps/admin-insights
        run: |
          vercel pull --yes --environment=production --token ${{ secrets.VERCEL_TOKEN }}
          vercel build --token ${{ secrets.VERCEL_TOKEN }}
          vercel deploy --prebuilt --prod --token ${{ secrets.VERCEL_TOKEN }}
          
      - name: Deploy Dev Portal
        if: ${{ vars.VERCEL_ORG_ID != '' && secrets.VERCEL_TOKEN != '' }}
        working-directory: apps/dev-portal
        run: |
          vercel pull --yes --environment=production --token ${{ secrets.VERCEL_TOKEN }}
          vercel build --token ${{ secrets.VERCEL_TOKEN }}
          vercel deploy --prebuilt --prod --token ${{ secrets.VERCEL_TOKEN }}

  # Update LIVE_URLS and emit success
  success:
    runs-on: ubuntu-latest
    needs: [test-basic, test-pro, deploy-frontends]
    if: always() && needs.test-basic.result == 'success' && needs.test-pro.result == 'success'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Update LIVE_URLS.json
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          cat > LIVE_URLS.json << EOF
          {
            "status": "CI_GREEN_DEPLOYED_BASIC&PRO",
            "tag": "prism-production-gates-$TIMESTAMP",
            "frontends": {
              "proof_messenger": "https://atlas-proof-messenger.vercel.app",
              "admin_insights": "https://atlas-admin-insights.vercel.app",
              "dev_portal": "https://atlas-dev-portal.vercel.app"
            },
            "backends": {
              "gateway": "https://atlas-gateway.sonthenguyen186.workers.dev",
              "witnesses": ["https://atlas-witness-1.sonthenguyen186.workers.dev", "https://atlas-witness-2.sonthenguyen186.workers.dev"]
            }
          }
          EOF
          echo "Updated LIVE_URLS.json"
          cat LIVE_URLS.json
          
      - name: Commit updated URLs
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add LIVE_URLS.json
          git commit -m "Update LIVE_URLS.json with Prism production gates verification" || exit 0
          git push origin main || exit 0
          
      - name: Emit Success JSON
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M)
          COMMIT_SHA="${{ github.sha }}"
          
          cat << EOF
          {
            "status": "LIVE_PRISM_CI_GREEN",
            "runs": {
              "actions": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            },
            "frontends": {
              "proof_messenger": "https://atlas-proof-messenger.vercel.app",
              "admin_insights": "https://atlas-admin-insights.vercel.app",
              "dev_portal": "https://atlas-dev-portal.vercel.app"
            },
            "gateway": "https://atlas-gateway.sonthenguyen186.workers.dev",
            "tests": {
              "playwright": "PASS",
              "lighthouse": {
                "basic": {"performance": ">=90", "accessibility": ">=95", "best-practices": ">=95", "seo": ">=95"},
                "pro": {"performance": ">=95", "accessibility": ">=95", "best-practices": "=100", "seo": "=100"}
              },
              "k6": {"p95_ms": "<200", "error_pct": "<1"}
            },
            "evidence": "docs/evidence/$TIMESTAMP/",
            "commit": "$COMMIT_SHA"
          }
          EOF
